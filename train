#!/usr/bin/env python3

import sys

sys.path.append('.')
import os
import time
from posixpath import join as pjoin

from torch.utils.data.dataloader import DataLoader

import option_parser
from datasets import create_dataset, get_character_names
from models import create_model
from option_parser import try_mkdir


def main():
    args = option_parser.get_args()
    characters = get_character_names(args)

    log_path = pjoin(args.save_dir, 'logs/')
    try_mkdir(args.save_dir)
    try_mkdir(log_path)

    with open(pjoin(args.save_dir, 'para.txt'), 'w') as para_file:
        para_file.write(' '.join(sys.argv))
    models = []
    print(characters)
    for c, char in enumerate(characters):
        dataset = create_dataset(args, char)
        # https://github.com/fastai/fastbook/issues/85
        # You always need to set num_workers=0 when creating a DataLoaders because Pytorch multiprocessing does not work on Windows.
        data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)

        models.append(create_model(args, char, dataset))

        if args.epoch_begin:
            models[c].load(c, epoch=args.epoch_begin, download=False)

        models[c].setup()

        start_time = time.time()
        for epoch in range(args.epoch_begin, args.epoch_num):
            for step, motions in enumerate(data_loader):
                models[c].set_input(motions)
                models[c].optimize_parameters()

                if args.verbose:
                    res = models[c].verbose()
                    print(f'[{epoch}/{args.epoch_num}]\t[{step}/{len(data_loader), res}]\t')

            if epoch % 50 == 0 or epoch == args.epoch_num - 1:
                models[c].save(c)

            models[c].epoch()

        end_tiem = time.time()
        print('training time', end_tiem - start_time)


if __name__ == '__main__':
    main()
