#!/usr/bin/env python3

import sys

sys.path.append('.')
import os
import time
from posixpath import join as pjoin

from torch.utils.data.dataloader import DataLoader

import option_parser
from datasets import create_dataset, get_character_names
from models import create_model
from option_parser import try_mkdir


def main():
    args = option_parser.get_args()
    characters = get_character_names(args)

    log_path = pjoin(args.save_dir, 'logs/')
    try_mkdir(args.save_dir)
    try_mkdir(log_path)

    with open(pjoin(args.save_dir, 'para.txt'), 'w') as para_file:
        para_file.write(' '.join(sys.argv))
    
    for c in range(len(characters)):
        char = characters[c]
        print(char)    
        dataset = create_dataset(args, char)
        # https://github.com/fastai/fastbook/issues/85
        # You always need to set num_workers=0 when creating a DataLoaders because Pytorch multiprocessing does not work on Windows.
        data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)

        model = create_model(args, char, dataset)

        if args.epoch_begin:
            model.load(epoch=args.epoch_begin, download=False)

        model.setup()

        start_time = time.time()
        for epoch in range(args.epoch_begin, args.epoch_num):
            for step, motions in enumerate(data_loader): 
                model.set_input(motions)
                model.optimize_parameters()

                if args.verbose:
                    res = model.verbose()
                    print(f'[{epoch}/{args.epoch_num}]\t[{step}/{len(data_loader), res}]\t')

            if epoch % 50 == 0 or epoch == args.epoch_num - 1:
                model.save(c)

            model.epoch()

        end_time = time.time()
        print('training time', end_time - start_time)


if __name__ == '__main__':
    main()
